import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import json
import os
from datetime import datetime
import base64
import requests
from io import BytesIO

# config.pyì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
from config import AppConfig
# data_handler.pyì—ì„œ ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤ ê°€ì ¸ì˜¤ê¸°
from data_handler import DataProcessor
# analysis_engine.pyì—ì„œ ë¶„ì„ ì—”ì§„ ê°€ì ¸ì˜¤ê¸°
from analysis_engine import BusinessAnalyzer


# Streamlit ì„¤ì •
st.set_page_config(**AppConfig.PAGE_CONFIG)

class SujeonggwaMarketAnalyzer:
    def __init__(self):
        self.required_columns = AppConfig.REQUIRED_COLUMNS
        self.our_brand = AppConfig.OUR_BRAND
        self.data_processor = DataProcessor()  # ë°ì´í„° ì²˜ë¦¬ê¸°
        self.business_analyzer = BusinessAnalyzer()  # ë¶„ì„ ì—”ì§„ ì¶”ê°€
    

    def load_latest_analysis_from_github(self):
        """GitHubì—ì„œ ìµœì‹  ë¶„ì„ ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸°"""
        github_config = AppConfig.get_github_config()
        github_token = github_config['token']
        github_api_url = AppConfig.get_github_api_url()
        
        if not github_token:
            return None
        
        try:
            headers = {
                "Authorization": f"token {github_token}",
                "Accept": "application/vnd.github.v3+json"
            }
            
            response = requests.get(github_api_url, headers=headers)
            
            if response.status_code == 200:
                files = response.json()
                
                analysis_files = [f for f in files if f['name'].startswith('analysis_results') and f['name'].endswith('.json')]
                
                if analysis_files:
                    latest_file = max(analysis_files, key=lambda x: x['name'])
                    file_response = requests.get(latest_file['download_url'])
                    
                    if file_response.status_code == 200:
                        return json.loads(file_response.text)
                
            return None
            
        except Exception as e:
            st.error(f"GitHubì—ì„œ ë¶„ì„ ê²°ê³¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return None

    def clear_github_results(self):
        """GitHubì—ì„œ ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ íŒŒì¼ë“¤ ì‚­ì œ"""
        github_config = AppConfig.get_github_config()
        github_token = github_config['token']
        github_api_url = AppConfig.get_github_api_url()
        
        if not github_token:
            return False
        
        try:
            headers = {
                "Authorization": f"token {github_token}",
                "Accept": "application/vnd.github.v3+json"
            }
            
            response = requests.get(github_api_url, headers=headers)
            
            if response.status_code == 200:
                files = response.json()
                analysis_files = [f for f in files if f['name'].startswith('analysis_results') and f['name'].endswith('.json')]
                
                for file_info in analysis_files:
                    delete_url = f"{github_api_url}/{file_info['name']}"
                    delete_data = {
                        "message": f"Delete old analysis result: {file_info['name']}",
                        "sha": file_info['sha']
                    }
                    
                    delete_response = requests.delete(delete_url, headers=headers, json=delete_data)
                    if delete_response.status_code != 200:
                        st.warning(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {file_info['name']}")
                
                return True
            
        except Exception as e:
            st.error(f"GitHub íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False

    def save_to_github(self, content, filename):
        """GitHubì— ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        github_config = AppConfig.get_github_config()
        github_token = github_config['token']
        github_api_url = AppConfig.get_github_api_url()
        
        if not github_token:
            return False
        
        try:
            content_encoded = base64.b64encode(content.encode('utf-8')).decode()
            
            url = f"{github_api_url}/{filename}"
            headers = {
                "Authorization": f"token {github_token}",
                "Accept": "application/vnd.github.v3+json"
            }
            
            data = {
                "message": f"ğŸ“Š ìˆ˜ì •ê³¼ ì‹œì¥ ë¶„ì„ ê²°ê³¼ ì—…ë°ì´íŠ¸: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
                "content": content_encoded,
            }
            
            response = requests.put(url, headers=headers, json=data)
            
            if response.status_code in [200, 201]:
                return True
            else:
                st.error(f"GitHub ì—…ë¡œë“œ ì‹¤íŒ¨: {response.status_code}")
                return False
                
        except Exception as e:
            st.error(f"GitHub ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False

def show_analysis_results(analysis_results, json_content, timestamp, github_success):
    """ë¶„ì„ ê²°ê³¼ë¥¼ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜"""
    
    if not analysis_results:
        st.error("ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    if github_success:
        st.success("âœ… ë¶„ì„ ì™„ë£Œ ë° GitHub ì €ì¥ ì„±ê³µ!")
    else:
        st.warning("âš ï¸ ë¶„ì„ ì™„ë£Œ, GitHub ì €ì¥ ì‹¤íŒ¨")
    
    tab_handmade, tab_all = st.tabs(["ğŸ¥› ìˆ˜ì œ ì œí’ˆ ë¶„ì„", "ğŸ­ ì „ì²´ ì œí’ˆ ë¶„ì„ (ìˆ˜ì œ+ê³µì¥í˜•)"])
    
    with tab_handmade:
        show_category_analysis(analysis_results.get('handmade_category', {}), "ìˆ˜ì œ")
    
    with tab_all:
        show_category_analysis(analysis_results.get('all_category', {}), "ì „ì²´")

def show_category_analysis(category_data, category_type):
    """ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ ê²°ê³¼ í‘œì‹œ"""
    
    if not category_data:
        st.warning(f"{category_type} ì¹´í…Œê³ ë¦¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í•µì‹¬ ì§€í‘œ ì¹´ë“œ
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ğŸ“Š ë¶„ì„ëœ ì œí’ˆ ìˆ˜", f"{category_data.get('total_products_analyzed', 0)}ê°œ")
    
    with col2:
        st.metric("ğŸ¯ ê³ ìœ  ì œí’ˆ ìˆ˜", f"{category_data.get('total_unique_products', 0)}ê°œ")
    
    with col3:
        our_count = category_data.get('our_unique_products_count', 0)
        st.metric("ğŸ¥¤ ì„œë¡œ ë¸Œëœë“œ", f"{our_count}ê°œ")
    
    with col4:
        competitor_count = category_data.get('competitor_unique_products_count', 0)
        st.metric("ğŸ­ ê²½ìŸì‚¬ ì œí’ˆ", f"{competitor_count}ê°œ")
    
    st.markdown("---")
    
    # í†µí•©ëœ ìš°ë¦¬ ì œí’ˆ í˜„í™©
    st.subheader(f"ğŸ¥¤ ì„œë¡œ ë¸Œëœë“œ ì¢…í•© í˜„í™© ({category_type})")
    
    business_insights = category_data.get('business_insights', {})
    
    # 1. ì œí’ˆë³„ ìƒì„¸ í˜„í™©
    st.markdown("### ğŸ“Š ì œí’ˆë³„ ìƒì„¸ í˜„í™©")
    if 'our_product_details' in business_insights:
        product_details = business_insights['our_product_details']
        
        if product_details:
            details_df = pd.DataFrame(product_details)
            st.dataframe(details_df, use_container_width=True)
            st.info(f"ğŸ’¡ ì´ {len(product_details)}ê°œì˜ ì„œë¡œ ë¸Œëœë“œ ì œí’ˆì´ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            st.warning("ì„œë¡œ ë¸Œëœë“œ ì œí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.warning("ì œí’ˆ ìƒì„¸ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    st.markdown("---")
    
    # 2. ì œí’ˆë³„ ê°€ê²© ê²½ìŸë ¥
    st.markdown("### ğŸ’° ì œí’ˆë³„ ê°€ê²© ê²½ìŸë ¥")
    if 'detailed_competitiveness' in business_insights:
        comp_data = business_insights['detailed_competitiveness']
        
        if comp_data:
            for platform, products in comp_data.items():
                with st.expander(f"ğŸª {platform} - {len(products)}ê°œ ì œí’ˆ"):
                    
                    for product in products:
                        st.markdown(f"**{product.get('ì œí’ˆ', 'N/A')}**")
                        
                        # ë¹„êµ ê¸°ì¤€ í‘œì‹œ
                        comparison_basis = product.get('ë¹„êµ_ê¸°ì¤€', 'N/A')
                        if comparison_basis == "ë™ì¼ ìš©ëŸ‰+ê°œìˆ˜":
                            st.success(f"ğŸ¯ **ë¹„êµ ê¸°ì¤€**: {comparison_basis}")
                        elif "ìœ ì‚¬ ìš©ëŸ‰" in comparison_basis:
                            st.info(f"ğŸ“Š **ë¹„êµ ê¸°ì¤€**: {comparison_basis}")
                        elif comparison_basis == "ë™ì¼ ê°œìˆ˜":
                            st.warning(f"ğŸ“ˆ **ë¹„êµ ê¸°ì¤€**: {comparison_basis}")
                        else:
                            st.error(f"ğŸ’° **ë¹„êµ ê¸°ì¤€**: {comparison_basis}")
                        
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            st.metric("ìš°ë¦¬ ë‹¨ìœ„ê°€ê²©", product.get('ìš°ë¦¬_ë‹¨ìœ„ê°€ê²©', 'N/A'))
                            st.metric("ê²½ìŸì‚¬ í‰ê· ", product.get('ê²½ìŸì‚¬_í‰ê· ', 'N/A'))
                        
                        with col2:
                            st.metric("ê²½ìŸì‚¬ ìµœì €", product.get('ê²½ìŸì‚¬_ìµœì €', 'N/A'))
                            st.metric("ê²½ìŸì‚¬ ìµœê³ ", product.get('ê²½ìŸì‚¬_ìµœê³ ', 'N/A'))
                        
                        with col3:
                            st.metric("ê°€ê²© ì°¨ì´", product.get('ê°€ê²©ì°¨ì´', 'N/A'), product.get('ê°€ê²©ì°¨ì´_í¼ì„¼íŠ¸', 'N/A'))
                            
                            position = product.get('ì‹œì¥_í¬ì§€ì…˜', 'N/A')
                            competitor_count = product.get('ê²½ìŸì‚¬_ìˆ˜', 0)
                            
                            if "ğŸ¯" in position:
                                st.success(f"**{position}** (ê²½ìŸì‚¬ {competitor_count}ê°œ)")
                            elif "ğŸ“Š" in position:
                                st.info(f"**{position}** (ê²½ìŸì‚¬ {competitor_count}ê°œ)")
                            elif "ğŸ“ˆ" in position:
                                st.warning(f"**{position}** (ê²½ìŸì‚¬ {competitor_count}ê°œ)")
                            else:
                                st.error(f"**{position}** (ê²½ìŸì‚¬ {competitor_count}ê°œ)")
                        
                        # ì£¼ìš” ê²½ìŸì‚¬ í‘œì‹œ
                        main_competitors = product.get('ì£¼ìš”_ê²½ìŸì‚¬', [])
                        if main_competitors:
                            st.markdown("**ğŸ“‹ ì£¼ìš” ê²½ìŸì‚¬:**")
                            for i, competitor in enumerate(main_competitors, 1):
                                st.write(f"  {i}. {competitor}")
                        
                        st.markdown("---")
        else:
            st.info("ì œí’ˆë³„ ê²½ìŸë ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("ì œí’ˆë³„ ê²½ìŸë ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    st.markdown("---")
    
    # 3. ìš©ëŸ‰ë³„/ê°œìˆ˜ë³„ ì‹œì¥ í˜„í™©
    st.markdown("### ğŸ“Š ìš©ëŸ‰ë³„/ê°œìˆ˜ë³„ ì‹œì¥ í˜„í™©")
    if 'volume_count_market' in business_insights:
        market_data = business_insights['volume_count_market']
        
        if market_data:
            st.markdown("#### ğŸ”¥ ì¸ê¸° ìš©ëŸ‰/ê°œìˆ˜ ì¡°í•© (ìƒìœ„ 10ê°œ)")
            
            market_df = pd.DataFrame(market_data)
            st.dataframe(market_df, use_container_width=True)
            
            # ìš°ë¦¬ê°€ ì§„ì¶œí•˜ì§€ ì•Šì€ ì‹œì¥ ì°¾ê¸°
            untapped_markets = [item for item in market_data if item.get('ìš°ë¦¬_ì œí’ˆìˆ˜', 0) == 0]
            
            if untapped_markets:
                st.markdown("#### ğŸ’¡ ì§„ì¶œ ê¸°íšŒ ìˆëŠ” ì‹œì¥")
                for market in untapped_markets[:5]:
                    volume_count = market.get('ìš©ëŸ‰_ê°œìˆ˜', 'N/A')
                    total_products = market.get('ì´_ì œí’ˆìˆ˜', 0)
                    avg_price = market.get('í‰ê· _ë‹¨ìœ„ê°€ê²©', 'N/A')
                    st.info(f"**{volume_count}**: {total_products}ê°œ ì œí’ˆ, í‰ê·  ë‹¨ìœ„ê°€ê²© {avg_price}")
        else:
            st.warning("ìš©ëŸ‰ë³„ ì‹œì¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("ìš©ëŸ‰ë³„ ì‹œì¥ ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    st.markdown("---")
    
    # 4. ë¸Œëœë“œë³„ ì‹œì¥ ë¶„ì„
    st.markdown("### ğŸ† ë¸Œëœë“œë³„ ì‹œì¥ ì ìœ ìœ¨")
    
    if 'market_share' in business_insights:
        share_data = business_insights['market_share']
        
        if share_data:
            share_df = pd.DataFrame([
                {'ë¸Œëœë“œ': brand, 'ì œí’ˆ ìˆ˜': data.get('ì œí’ˆ_ìˆ˜', 0), 'ì ìœ ìœ¨': f"{data.get('ì ìœ ìœ¨_í¼ì„¼íŠ¸', 0)}%"}
                for brand, data in share_data.items()
            ])
            
            st.dataframe(share_df, use_container_width=True)
            
            # ì„œë¡œ ë¸Œëœë“œ ìˆœìœ„ ì°¾ê¸°
            seoro_rank = None
            for idx, (brand, _) in enumerate(share_data.items(), 1):
                if brand == "ì„œë¡œ":
                    seoro_rank = idx
                    break
            
            if seoro_rank:
                if seoro_rank == 1:
                    st.success(f"ğŸ† ì„œë¡œ ë¸Œëœë“œê°€ **1ìœ„**ì…ë‹ˆë‹¤!")
                elif seoro_rank <= 3:
                    st.info(f"ğŸ¥‰ ì„œë¡œ ë¸Œëœë“œê°€ **{seoro_rank}ìœ„**ì…ë‹ˆë‹¤.")
                else:
                    st.warning(f"ğŸ“ˆ ì„œë¡œ ë¸Œëœë“œê°€ **{seoro_rank}ìœ„**ì…ë‹ˆë‹¤. ë” ë§ì€ ì œí’ˆ ë¼ì¸ì—…ì´ í•„ìš”í•´ ë³´ì…ë‹ˆë‹¤.")
            else:
                st.info("ì„œë¡œ ë¸Œëœë“œëŠ” í˜„ì¬ ìƒìœ„ 10ìœ„ ì•ˆì— ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.warning("ë¸Œëœë“œë³„ ì ìœ ìœ¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("ë¸Œëœë“œë³„ ì ìœ ìœ¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

def main():
    # í—¤ë”
    st.title("ğŸ¥¤ ì„œë¡œ ìˆ˜ì •ê³¼ - ì‹œì¥ ê°€ê²© ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
    st.markdown("##### *í”Œë«í¼ë³„ ê°€ê²© ê²½ìŸë ¥ ë° ì‹œì¥ í¬ì§€ì…”ë‹ ë¶„ì„*")
    
    st.markdown("---")
    
    analyzer = SujeonggwaMarketAnalyzer()
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'run_analysis' not in st.session_state:
        st.session_state.run_analysis = False
    if 'analysis_results' not in st.session_state:
        st.session_state.analysis_results = None
    if 'json_content' not in st.session_state:
        st.session_state.json_content = None
    if 'timestamp' not in st.session_state:
        st.session_state.timestamp = None
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ğŸ“Š ë¶„ì„ ì„¤ì •")
        
        uploaded_files = st.file_uploader(
            "ì—‘ì…€ íŒŒì¼ ì—…ë¡œë“œ",
            type=['xlsx', 'xls'],
            accept_multiple_files=True,
            help="ë„¤ì´ë²„, ì¿ íŒ¡, ì˜¬ì›¨ì´ì¦ˆ ìˆ˜ì •ê³¼ ê°€ê²© ë°ì´í„°"
        )
        
        st.markdown("---")
        
        if uploaded_files:
            st.success(f"âœ… {len(uploaded_files)}ê°œ íŒŒì¼ ì—…ë¡œë“œë¨")
            for file in uploaded_files:
                platform = analyzer.extract_platform_from_filename(file.name)
                st.write(f"ğŸ“„ {platform}: {file.name}")

            # ë°ì´í„° í’ˆì§ˆ ì •ë³´ í‘œì‹œ (ìƒˆë¡œ ì¶”ê°€)
            with st.expander("ğŸ“Š ë°ì´í„° í’ˆì§ˆ í™•ì¸", expanded=False):
                temp_df_list = []
                for file in uploaded_files:
                    df, platform, missing_cols = analyzer.data_processor.load_and_standardize_excel(file)
                    if df is not None:
                        temp_df_list.append(df)
                
                if temp_df_list:
                    quality_info = analyzer.data_processor.validate_data_quality(temp_df_list)
                    st.write(f"ğŸ“ ì´ {quality_info['total_files']}ê°œ íŒŒì¼")
                    st.write(f"ğŸ“Š ì´ {quality_info['total_products']}ê°œ ì œí’ˆ")
                    st.write(f"ğŸª í”Œë«í¼: {', '.join(quality_info['platforms'])}")
                    
                    if quality_info['quality_issues']:
                        st.write("âš ï¸ í’ˆì§ˆ ì´ìŠˆ:")
                        for issue in quality_info['quality_issues']:
                            st.write(f"  â€¢ {issue}")

        
        
        if st.button("ğŸš€ ë¶„ì„ ì‹œì‘", type="primary", disabled=not uploaded_files):
            st.session_state.run_analysis = True
        
        st.markdown("---")
        st.markdown("### ğŸ“‹ ë¶„ì„ í•­ëª©")
        st.markdown("""
        - âœ… ì œí’ˆë³„ ê°€ê²© ê²½ìŸë ¥
        - âœ… ìš©ëŸ‰/ê°œìˆ˜ë³„ ì‹œì¥ ë¶„ì„  
        - âœ… ë¸Œëœë“œë³„ ì ìœ ìœ¨
        - âœ… ì§„ì¶œ ê¸°íšŒ ë°œê²¬
        """)
    

    # ë©”ì¸ ë¶„ì„
    if uploaded_files and st.session_state.get('run_analysis', False):
        
        # í”„ë¡œê·¸ë ˆìŠ¤ í‘œì‹œ
        progress_container = st.container()
        with progress_container:
            progress_bar = st.progress(0)
            status_text = st.empty()
        
        df_list = []
        platform_info = []
        
        # íŒŒì¼ ì²˜ë¦¬
        for i, uploaded_file in enumerate(uploaded_files):
            status_text.text(f"ğŸ“‚ íŒŒì¼ ì²˜ë¦¬ ì¤‘: {uploaded_file.name}")
            progress_bar.progress((i + 1) / len(uploaded_files) * 0.4)
            
            df, platform, missing_cols = analyzer.data_processor.load_and_standardize_excel(uploaded_file)
            
            if df is not None:
                df_list.append(df)
                platform_info.append({
                    'filename': uploaded_file.name,
                    'platform': platform,
                    'rows': len(df),
                    'missing_columns': missing_cols
                })
        
        if df_list:
            status_text.text("ğŸ” ì‹œì¥ ë°ì´í„° ë¶„ì„ ì¤‘...")
            progress_bar.progress(0.7)
            
            # í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„
            analysis_results, handmade_df, all_products_df = analyzer.business_analyzer.analyze_business_critical_data(df_list)
            
            if analysis_results:
                status_text.text("ğŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")
                progress_bar.progress(0.9)
                
                # GitHubì— ìë™ ì €ì¥
                status_text.text("ğŸ’¾ GitHubì— ì €ì¥ ì¤‘...")
                
                # ê¸°ì¡´ ê²°ê³¼ íŒŒì¼ë“¤ ì‚­ì œ
                analyzer.clear_github_results()
                
                # ìƒˆ ê²°ê³¼ ì €ì¥
                json_content = json.dumps(analysis_results, ensure_ascii=False, indent=2)
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                json_filename = f"analysis_results_{timestamp}.json"
                
                github_success = analyzer.save_to_github(json_content, json_filename)
                
                progress_bar.progress(1.0)
                status_text.empty()
                progress_container.empty()
                
                # ê²°ê³¼ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                st.session_state.analysis_results = analysis_results
                st.session_state.json_content = json_content
                st.session_state.timestamp = timestamp
                
                # ê²°ê³¼ ëŒ€ì‹œë³´ë“œ í‘œì‹œ
                show_analysis_results(analysis_results, json_content, timestamp, github_success)
            else:
                st.error("ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            
            # ì„¸ì…˜ ìƒíƒœ ë¦¬ì…‹
            st.session_state.run_analysis = False
        else:
            st.error("ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            st.session_state.run_analysis = False
    
    # ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ê°€ ì„¸ì…˜ì— ìˆê±°ë‚˜ GitHubì—ì„œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆëŠ” ê²½ìš°
    elif st.session_state.get('analysis_results') or not uploaded_files:
        
        # ì„¸ì…˜ì— ê²°ê³¼ê°€ ì—†ìœ¼ë©´ GitHubì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
        if not st.session_state.get('analysis_results'):
            with st.spinner("GitHubì—ì„œ ìµœì‹  ë¶„ì„ ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
                latest_analysis = analyzer.load_latest_analysis_from_github()
                
                if latest_analysis:
                    st.session_state.analysis_results = latest_analysis
                    st.session_state.json_content = json.dumps(latest_analysis, ensure_ascii=False, indent=2)
                    st.session_state.timestamp = latest_analysis.get('timestamp', 'unknown')
                    st.success("âœ… GitHubì—ì„œ ìµœì‹  ë¶„ì„ ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤!")
        
        # ë¶„ì„ ê²°ê³¼ í‘œì‹œ
        if st.session_state.get('analysis_results'):
            show_analysis_results(
                st.session_state.analysis_results, 
                st.session_state.get('json_content', ''), 
                st.session_state.get('timestamp', 'unknown'),
                True
            )
        else:
            # ì´ˆê¸° í™”ë©´  
            st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ì—‘ì…€ íŒŒì¼ë“¤ì„ ì—…ë¡œë“œí•˜ê³  ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.")
            
            # ê°„ë‹¨í•œ ì•ˆë‚´ ë©”ì‹œì§€
            with st.expander("ğŸ“‹ ì‚¬ìš© ë°©ë²•", expanded=False):
                st.markdown("""
                ### ğŸš€ ì£¼ìš” ê¸°ëŠ¥
                
                **ğŸ¯ í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„**
                - ì„œë¡œ ë¸Œëœë“œì˜ ì œí’ˆë³„ ê°€ê²© ê²½ìŸë ¥ ë¶„ì„
                - ìš©ëŸ‰/ê°œìˆ˜ë³„ ì„¸ë¶„í™”ëœ ì‹œì¥ í¬ì§€ì…”ë‹  
                - ê²½ìŸì‚¬ ëŒ€ë¹„ ì •í™•í•œ ê°€ê²© ì°¨ì´ ë¶„ì„
                
                **ğŸ“Š ì‹œì¥ í˜„í™© íŒŒì•…**
                - ìš©ëŸ‰ë³„/ê°œìˆ˜ë³„ ì¸ê¸° ì‹œì¥ ë°œê²¬
                - ì§„ì¶œ ê¸°íšŒ ìˆëŠ” ì‹œì¥ ìë™ ì¶”ì²œ
                - ë¸Œëœë“œë³„ ì‹œì¥ ì ìœ ìœ¨ ë¶„ì„
                
                ### ğŸ“ íŒŒì¼ ì—…ë¡œë“œ ê°€ì´ë“œ
                - **ì§€ì› í˜•ì‹**: Excel íŒŒì¼ (.xlsx, .xls)
                - **íŒŒì¼ëª… ì˜ˆì‹œ**: "ë„¤ì´ë²„ ìˆ˜ì •ê³¼ ê°€ê²©", "ì¿ íŒ¡ ìˆ˜ì •ê³¼ ê°€ê²©" ë“±
                - **í•„ìˆ˜ ì»¬ëŸ¼**: ë¸Œëœë“œ, ì œí’ˆëª…, ìš©ëŸ‰(ml), ê°œìˆ˜, ìµœì €ê°€(ë°°ì†¡ë¹„ í¬í•¨), ìµœì €ê°€ ë‹¨ìœ„ê°€ê²©(100mlë‹¹) ë“±
                """)

# Streamlit ì•± ì‹¤í–‰
if __name__ == "__main__":
    main()
