import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import json
import os
from datetime import datetime
import base64
import requests
from io import BytesIO

# GitHub ì„¤ì •
GITHUB_TOKEN = st.secrets.get("GITHUB_TOKEN", "")
GITHUB_REPO = st.secrets.get("GITHUB_REPO", "coder4052/market_analysis")
GITHUB_API_URL = f"https://api.github.com/repos/{GITHUB_REPO}/contents"

class SujeonggwaMarketAnalyzer:
    def __init__(self):
        self.required_columns = [
            "ë¸Œëœë“œ", "ì œí’ˆëª…", "ìš©ëŸ‰(ml)", "ê°œìˆ˜", 
            "ì¼ë°˜ íŒë§¤ê°€", "ì¼ë°˜ íŒë§¤ê°€ ë‹¨ìœ„ê°€ê²©(100mlë‹¹)", 
            "ìƒì‹œ í• ì¸ê°€", "ìƒì‹œ í• ì¸ê°€ ë‹¨ìœ„ê°€ê²©(100mlë‹¹)",
            "ë°°ì†¡ë¹„", "ìµœì €ê°€(ë°°ì†¡ë¹„ í¬í•¨)", "ìµœì €ê°€ ë‹¨ìœ„ê°€ê²©(100mlë‹¹)", 
            "ê³µì¥í˜• ì—¬ë¶€"
        ]
        self.our_brand = "ì„œë¡œ"
    
    def extract_platform_from_filename(self, filename):
        """íŒŒì¼ëª…ì—ì„œ í”Œë«í¼ ì¶”ì¶œ"""
        filename_lower = filename.lower()
        if 'ë„¤ì´ë²„' in filename:
            return 'ë„¤ì´ë²„'
        elif 'ì¿ íŒ¡' in filename:
            return 'ì¿ íŒ¡'
        elif 'ì˜¬ì›¨ì´ì¦ˆ' in filename:
            return 'ì˜¬ì›¨ì´ì¦ˆ'
        else:
            return 'ê¸°íƒ€'
    
    def load_and_standardize_excel(self, uploaded_file):
        """ì—‘ì…€ íŒŒì¼ ë¡œë“œ ë° í‘œì¤€í™”"""
        try:
            df = pd.read_excel(uploaded_file, sheet_name=0)
            platform = self.extract_platform_from_filename(uploaded_file.name)
            
            # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
            available_columns = [col for col in self.required_columns if col in df.columns]
            missing_columns = [col for col in self.required_columns if col not in df.columns]
            
            if missing_columns:
                st.warning(f"[{platform}] ëˆ„ë½ëœ ì»¬ëŸ¼: {missing_columns}")
            
            # ë°ì´í„° ì •ì œ
            df_clean = df[available_columns].copy()
            df_clean['í”Œë«í¼'] = platform
            df_clean['ë¶„ì„_ì‹œê°„'] = datetime.now().strftime('%Y-%m-%d %H:%M')
            
            # ìˆ«ìí˜• ì»¬ëŸ¼ ë³€í™˜
            numeric_columns = ['ìš©ëŸ‰(ml)', 'ê°œìˆ˜', 'ìµœì €ê°€(ë°°ì†¡ë¹„ í¬í•¨)', 'ìµœì €ê°€ ë‹¨ìœ„ê°€ê²©(100mlë‹¹)', 'ê³µì¥í˜• ì—¬ë¶€']
            for col in numeric_columns:
                if col in df_clean.columns:
                    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
            
            return df_clean, platform, missing_columns
            
        except Exception as e:
            st.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return None, None, None
    
    def analyze_business_critical_data(self, df_list):
        """ì†Œìƒê³µì¸ ê´€ì ì˜ í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„"""
        if not df_list:
            return None
        
        combined_df = pd.concat(df_list, ignore_index=True)
        
        # 1ì°¨: ìˆ˜ì œ ì œí’ˆë§Œ í•„í„°ë§ (ê³µì¥í˜• ì—¬ë¶€ = 0)
        if 'ê³µì¥í˜• ì—¬ë¶€' in combined_df.columns:
            handmade_df = combined_df[combined_df['ê³µì¥í˜• ì—¬ë¶€'] == 0].copy()
            all_products_df = combined_df.copy()  # ì „ì²´ ì œí’ˆ (ìˆ˜ì œ + ê³µì¥í˜•)
        else:
            handmade_df = combined_df.copy()
            all_products_df = combined_df.copy()
            st.warning("'ê³µì¥í˜• ì—¬ë¶€' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ìˆ˜ì œ ì œí’ˆ ë¶„ì„
        handmade_analysis = self._analyze_category(handmade_df, "ìˆ˜ì œ ì œí’ˆ")
        
        # ì „ì²´ ì œí’ˆ ë¶„ì„ (ìˆ˜ì œ + ê³µì¥í˜•)
        all_analysis = self._analyze_category(all_products_df, "ì „ì²´ ì œí’ˆ")
        
        # í†µí•© ë¶„ì„ ê²°ê³¼
        analysis_results = {
            'timestamp': datetime.now().isoformat(),
            'analysis_type': 'ìˆ˜ì •ê³¼ ì‹œì¥ ë¶„ì„',
            'our_brand': self.our_brand,
            'handmade_category': handmade_analysis,
            'all_category': all_analysis,
            'platforms_analyzed': combined_df['í”Œë«í¼'].unique().tolist()
        }
        
        return analysis_results, handmade_df, all_products_df
    
    def _analyze_category(self, df, category_name):
        """ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ (ìˆ˜ì œ ë˜ëŠ” ì „ì²´)"""
        
        # ì„œë¡œ ë¸Œëœë“œ ë°ì´í„° ì¶”ì¶œ
        our_products = df[df['ë¸Œëœë“œ'] == self.our_brand].copy()
        competitor_products = df[df['ë¸Œëœë“œ'] != self.our_brand].copy()
        
        # ì œí’ˆ ê·¸ë£¹í•‘ (ë¸Œëœë“œ + ì œí’ˆëª…ì´ ê°™ìœ¼ë©´ ê°™ì€ ì œí’ˆìœ¼ë¡œ ì²˜ë¦¬)
        unique_products = df.groupby(['ë¸Œëœë“œ', 'ì œí’ˆëª…']).agg({
            'ìš©ëŸ‰(ml)': lambda x: list(x.unique()),  # ìš©ëŸ‰ ë¦¬ìŠ¤íŠ¸
            'ê°œìˆ˜': 'first',  # ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
            'ìµœì €ê°€(ë°°ì†¡ë¹„ í¬í•¨)': 'min',  # ìµœì €ê°€ ì„ íƒ
            'ìµœì €ê°€ ë‹¨ìœ„ê°€ê²©(100mlë‹¹)': 'min',  # ìµœì € ë‹¨ìœ„ê°€ê²© ì„ íƒ
            'í”Œë«í¼': lambda x: list(x.unique())  # íŒë§¤ í”Œë«í¼ ë¦¬ìŠ¤íŠ¸
        }).reset_index()
        
        # ìš°ë¦¬ ë¸Œëœë“œ ê³ ìœ  ì œí’ˆ ìˆ˜ ê³„ì‚°
        our_unique_products = unique_products[unique_products['ë¸Œëœë“œ'] == self.our_brand]
        competitor_unique_products = unique_products[unique_products['ë¸Œëœë“œ'] != self.our_brand]
        
        category_results = {
            'category_name': category_name,
            'total_products_analyzed': len(df),
            'total_unique_products': len(unique_products),
            'our_products_count': len(our_products),
            'our_unique_products_count': len(our_unique_products),
            'competitor_products_count': len(competitor_products),
            'competitor_unique_products_count': len(competitor_unique_products),
            'business_insights': {}
        }
        
        # 1. ìš°ë¦¬ ë¸Œëœë“œ í”Œë«í¼ë³„ í˜„í™© (ê³ ìœ  ì œí’ˆ ê¸°ì¤€)
        if not our_unique_products.empty:
            our_platform_status = {}
            for platform in df['í”Œë«í¼'].unique():
                # í•´ë‹¹ í”Œë«í¼ì—ì„œ íŒë§¤ë˜ëŠ” ìš°ë¦¬ ë¸Œëœë“œ ê³ ìœ  ì œí’ˆ
                platform_products = our_unique_products[
                    our_unique_products['í”Œë«í¼'].apply(lambda x: platform in x)
                ]
                
                if not platform_products.empty:
                    our_platform_status[platform] = {
                        'ê³ ìœ ì œí’ˆ_ìˆ˜': len(platform_products),
                        'í‰ê· _ìµœì €ê°€': round(platform_products['ìµœì €ê°€(ë°°ì†¡ë¹„ í¬í•¨)'].mean(), 0),
                        'í‰ê· _ë‹¨ìœ„ê°€ê²©': round(platform_products['ìµœì €ê°€ ë‹¨ìœ„ê°€ê²©(100mlë‹¹)'].mean(), 0),
                        'ìš©ëŸ‰_ì¢…ë¥˜': len(set([vol for volumes in platform_products['ìš©ëŸ‰(ml)'] for vol in volumes]))
                    }
            
            category_results['business_insights']['our_platform_status'] = our_platform_status
        
        # 2. í”Œë«í¼ë³„ ê°€ê²© ê²½ìŸë ¥ ë¶„ì„ (ê³ ìœ  ì œí’ˆ ê¸°ì¤€)
        if 'ìµœì €ê°€ ë‹¨ìœ„ê°€ê²©(100mlë‹¹)' in df.columns:
            competitiveness = {}
            for platform in df['í”Œë«í¼'].unique():
                platform_data = df[df['í”Œë«í¼'] == platform]
                our_platform_data = our_products[our_products['í”Œë«í¼'] == platform]
                competitor_platform_data = competitor_products[competitor_products['í”Œë«í¼'] == platform]
                
                if not our_platform_data.empty and not competitor_platform_data.empty:
                    our_avg_unit_price = our_platform_data['ìµœì €ê°€ ë‹¨ìœ„ê°€ê²©(100mlë‹¹)'].mean()
                    competitor_avg_unit_price = competitor_platform_data['ìµœì €ê°€ ë‹¨ìœ„ê°€ê²©(100mlë‹¹)'].mean()
                    competitor_min_unit_price = competitor_platform_data['ìµœì €ê°€ ë‹¨ìœ„ê°€ê²©(100mlë‹¹)'].min()
                    competitor_max_unit_price = competitor_platform_data['ìµœì €ê°€ ë‹¨ìœ„ê°€ê²©(100mlë‹¹)'].max()
                    
                    price_gap = our_avg_unit_price - competitor_avg_unit_price
                    price_gap_percent = (price_gap / competitor_avg_unit_price) * 100
                    
                    # ì‹œì¥ ìœ„ì¹˜ íŒë‹¨
                    if our_avg_unit_price <= competitor_min_unit_price:
                        market_position = "ìµœì €ê°€ ê·¸ë£¹"
                    elif our_avg_unit_price <= competitor_avg_unit_price:
                        market_position = "í‰ê·  ì´í•˜"
                    elif our_avg_unit_price <= competitor_max_unit_price:
                        market_position = "í‰ê·  ì´ìƒ"
                    else:
                        market_position = "ìµœê³ ê°€ ê·¸ë£¹"
                    
                    competitiveness[platform] = {
                        'ìš°ë¦¬_í‰ê· ë‹¨ìœ„ê°€ê²©': round(our_avg_unit_price, 0),
                        'ê²½ìŸì‚¬_í‰ê· ë‹¨ìœ„ê°€ê²©': round(competitor_avg_unit_price, 0),
                        'ê²½ìŸì‚¬_ìµœì €ë‹¨ìœ„ê°€ê²©': round(competitor_min_unit_price, 0),
                        'ê²½ìŸì‚¬_ìµœê³ ë‹¨ìœ„ê°€ê²©': round(competitor_max_unit_price, 0),
                        'ê°€ê²©ì°¨ì´': round(price_gap, 0),
                        'ê°€ê²©ì°¨ì´_í¼ì„¼íŠ¸': round(price_gap_percent, 1),
                        'ì‹œì¥_í¬ì§€ì…˜': market_position
                    }
            
            category_results['business_insights']['price_competitiveness'] = competitiveness
        
        # 3. ë¸Œëœë“œë³„ ì‹œì¥ ì ìœ ìœ¨ (ê³ ìœ  ì œí’ˆ ìˆ˜ ê¸°ì¤€)
        brand_share = unique_products['ë¸Œëœë“œ'].value_counts()
        total_unique_products = len(unique_products)
        brand_share_percent = {}
        
        for brand, count in brand_share.head(10).items():
            percentage = (count / total_unique_products) * 100
            brand_share_percent[brand] = {
                'ê³ ìœ ì œí’ˆ_ìˆ˜': int(count),
                'ì ìœ ìœ¨_í¼ì„¼íŠ¸': round(percentage, 1)
            }
        
        category_results['business_insights']['market_share'] = brand_share_percent
        
        # 4. ê°€ê²©ëŒ€ë³„ ì œí’ˆ ë¶„í¬ (ìš°ë¦¬ ë¸Œëœë“œ ìœ„ì¹˜ íŒŒì•…ìš©)
        if 'ìµœì €ê°€ ë‹¨ìœ„ê°€ê²©(100mlë‹¹)' in df.columns:
            unit_prices = df['ìµœì €ê°€ ë‹¨ìœ„ê°€ê²©(100mlë‹¹)'].dropna()
            
            # ê°€ê²©ëŒ€ êµ¬ê°„ ìƒì„±
            price_ranges = {
                '1000ì› ë¯¸ë§Œ': len(unit_prices[unit_prices < 1000]),
                '1000-2000ì›': len(unit_prices[(unit_prices >= 1000) & (unit_prices < 2000)]),
                '2000-3000ì›': len(unit_prices[(unit_prices >= 2000) & (unit_prices < 3000)]),
                '3000-4000ì›': len(unit_prices[(unit_prices >= 3000) & (unit_prices < 4000)]),
                '4000ì› ì´ìƒ': len(unit_prices[unit_prices >= 4000])
            }
            
            # ìš°ë¦¬ ì œí’ˆì´ ì†í•œ ê°€ê²©ëŒ€ í‘œì‹œ
            our_price_distribution = {}
            if not our_products.empty and 'ìµœì €ê°€ ë‹¨ìœ„ê°€ê²©(100mlë‹¹)' in our_products.columns:
                our_unit_prices = our_products['ìµœì €ê°€ ë‹¨ìœ„ê°€ê²©(100mlë‹¹)'].dropna()
                for price_range, count in price_ranges.items():
                    if price_range == '1000ì› ë¯¸ë§Œ':
                        our_count = len(our_unit_prices[our_unit_prices < 1000])
                    elif price_range == '1000-2000ì›':
                        our_count = len(our_unit_prices[(our_unit_prices >= 1000) & (our_unit_prices < 2000)])
                    elif price_range == '2000-3000ì›':
                        our_count = len(our_unit_prices[(our_unit_prices >= 2000) & (our_unit_prices < 3000)])
                    elif price_range == '3000-4000ì›':
                        our_count = len(our_unit_prices[(our_unit_prices >= 3000) & (our_unit_prices < 4000)])
                    else:  # 4000ì› ì´ìƒ
                        our_count = len(our_unit_prices[our_unit_prices >= 4000])
                    
                    our_price_distribution[price_range] = {
                        'ì „ì²´_ì œí’ˆìˆ˜': count,
                        'ìš°ë¦¬_ì œí’ˆìˆ˜': our_count
                    }
            
            category_results['business_insights']['price_distribution'] = our_price_distribution
        
        return category_results

    def load_latest_analysis_from_github(self):
        """GitHubì—ì„œ ìµœì‹  ë¶„ì„ ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸°"""
        if not GITHUB_TOKEN:
            return None
        
        try:
            headers = {
                "Authorization": f"token {GITHUB_TOKEN}",
                "Accept": "application/vnd.github.v3+json"
            }
            
            # ì €ì¥ì†Œ ë‚´ìš© ì¡°íšŒ
            response = requests.get(GITHUB_API_URL, headers=headers)
            
            if response.status_code == 200:
                files = response.json()
                
                # ë¶„ì„ ê²°ê³¼ íŒŒì¼ë“¤ ì°¾ê¸°
                analysis_files = [f for f in files if f['name'].startswith('analysis_results') and f['name'].endswith('.json')]
                
                if analysis_files:
                    # ìµœì‹  íŒŒì¼ ì„ íƒ (íŒŒì¼ëª…ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€)
                    latest_file = max(analysis_files, key=lambda x: x['name'])
                    
                    # íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                    file_response = requests.get(latest_file['download_url'])
                    
                    if file_response.status_code == 200:
                        return json.loads(file_response.text)
                
            return None
            
        except Exception as e:
            st.error(f"GitHubì—ì„œ ë¶„ì„ ê²°ê³¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return None

    def clear_github_results(self):
        """GitHubì—ì„œ ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ íŒŒì¼ë“¤ ì‚­ì œ"""
        if not GITHUB_TOKEN:
            return False
        
        try:
            headers = {
                "Authorization": f"token {GITHUB_TOKEN}",
                "Accept": "application/vnd.github.v3+json"
            }
            
            # ì €ì¥ì†Œ ë‚´ìš© ì¡°íšŒ
            response = requests.get(GITHUB_API_URL, headers=headers)
            
            if response.status_code == 200:
                files = response.json()
                
                # ë¶„ì„ ê²°ê³¼ íŒŒì¼ë“¤ ì°¾ê¸° (analysis_resultsë¡œ ì‹œì‘í•˜ëŠ” JSON íŒŒì¼)
                analysis_files = [f for f in files if f['name'].startswith('analysis_results') and f['name'].endswith('.json')]
                
                # ê° íŒŒì¼ ì‚­ì œ
                for file_info in analysis_files:
                    delete_url = f"{GITHUB_API_URL}/{file_info['name']}"
                    delete_data = {
                        "message": f"Delete old analysis result: {file_info['name']}",
                        "sha": file_info['sha']
                    }
                    
                    delete_response = requests.delete(delete_url, headers=headers, json=delete_data)
                    if delete_response.status_code != 200:
                        st.warning(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {file_info['name']}")
                
                return True
            
        except Exception as e:
            st.error(f"GitHub íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False

    def save_to_github(self, content, filename):
        """GitHubì— ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        if not GITHUB_TOKEN:
            st.error("""
            ğŸ”§ **GitHub ì—°ë™ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤**
            
            **Streamlit Cloudì—ì„œ ì„¤ì •í•˜ëŠ” ë°©ë²•:**
            1. ì•± ëŒ€ì‹œë³´ë“œ â†’ Settings â†’ Secrets
            2. ë‹¤ìŒ ë‚´ìš© ì¶”ê°€:
            ```
            GITHUB_TOKEN = "your_github_token"
            GITHUB_REPO = "username/repo-name"
            ```
            """)
            return False
        
        try:
            content_encoded = base64.b64encode(content.encode('utf-8')).decode()
            
            url = f"{GITHUB_API_URL}/{filename}"
            headers = {
                "Authorization": f"token {GITHUB_TOKEN}",
                "Accept": "application/vnd.github.v3+json"
            }
            
            data = {
                "message": f"ğŸ“Š ìˆ˜ì •ê³¼ ì‹œì¥ ë¶„ì„ ê²°ê³¼ ì—…ë°ì´íŠ¸: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
                "content": content_encoded,
            }
            
            response = requests.put(url, headers=headers, json=data)
            
            if response.status_code in [200, 201]:
                return True
            else:
                st.error(f"GitHub ì—…ë¡œë“œ ì‹¤íŒ¨: {response.status_code} - {response.text}")
                return False
                
        except Exception as e:
            st.error(f"GitHub ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False

def show_analysis_results(analysis_results, json_content, timestamp, github_success):
    """ë¶„ì„ ê²°ê³¼ë¥¼ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜"""
    
    # ê²°ê³¼ ëŒ€ì‹œë³´ë“œ
    if github_success:
        st.success("âœ… ë¶„ì„ ì™„ë£Œ ë° GitHub ì €ì¥ ì„±ê³µ!")
    else:
        st.warning("âš ï¸ ë¶„ì„ ì™„ë£Œ, GitHub ì €ì¥ ì‹¤íŒ¨")
    
    # ì¹´í…Œê³ ë¦¬ ì„ íƒ íƒ­
    tab_handmade, tab_all = st.tabs(["ğŸ¥› ìˆ˜ì œ ì œí’ˆ ë¶„ì„", "ğŸ­ ì „ì²´ ì œí’ˆ ë¶„ì„ (ìˆ˜ì œ+ê³µì¥í˜•)"])
    
    with tab_handmade:
        show_category_analysis(analysis_results.get('handmade_category', {}), "ìˆ˜ì œ")
    
    with tab_all:
        show_category_analysis(analysis_results.get('all_category', {}), "ì „ì²´")

def show_category_analysis(category_data, category_type):
    """ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ ê²°ê³¼ í‘œì‹œ"""
    
    if not category_data:
        st.warning(f"{category_type} ì¹´í…Œê³ ë¦¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í•µì‹¬ ì§€í‘œ ì¹´ë“œ
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ğŸ“Š ë¶„ì„ëœ ì œí’ˆ ìˆ˜", f"{category_data.get('total_products_analyzed', 0)}ê°œ")
    
    with col2:
        st.metric("ğŸ¯ ê³ ìœ  ì œí’ˆ ìˆ˜", f"{category_data.get('total_unique_products', 0)}ê°œ")
    
    with col3:
        our_count = category_data.get('our_unique_products_count', 0)
        st.metric("ğŸ¥¤ ì„œë¡œ ë¸Œëœë“œ (ê³ ìœ )", f"{our_count}ê°œ")
    
    with col4:
        competitor_count = category_data.get('competitor_unique_products_count', 0)
        st.metric("ğŸ­ ê²½ìŸì‚¬ ì œí’ˆ (ê³ ìœ )", f"{competitor_count}ê°œ")
    
    st.markdown("---")
    
    # ì„¸ë¶€ ë¶„ì„ íƒ­
    tab1, tab2, tab3 = st.tabs(["ğŸ’° ê°€ê²© ê²½ìŸë ¥", "ğŸ“Š ì‹œì¥ ë¶„ì„", "ğŸ“ˆ ìƒì„¸ ì •ë³´"])
    
    with tab1:
        st.subheader(f"ğŸ’° {category_type} ì¹´í…Œê³ ë¦¬ ê°€ê²© ê²½ìŸë ¥ ë¶„ì„")
        
        # ê²½ìŸë ¥ ìš”ì•½ í…Œì´ë¸”
        if 'price_competitiveness' in category_data.get('business_insights', {}):
            comp_data = category_data['business_insights']['price_competitiveness']
            
            st.markdown("#### ğŸ“‹ í”Œë«í¼ë³„ ê²½ìŸë ¥ ìš”ì•½")
            
            for platform, data in comp_data.items():
                with st.expander(f"ğŸª {platform} ìƒì„¸ ë¶„ì„"):
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.metric("ì„œë¡œ í‰ê·  ë‹¨ìœ„ê°€ê²©", f"{data['ìš°ë¦¬_í‰ê· ë‹¨ìœ„ê°€ê²©']:,}ì›")
                        st.metric("ê²½ìŸì‚¬ í‰ê·  ë‹¨ìœ„ê°€ê²©", f"{data['ê²½ìŸì‚¬_í‰ê· ë‹¨ìœ„ê°€ê²©']:,}ì›")
                    
                    with col2:
                        price_diff = data['ê°€ê²©ì°¨ì´']
                        price_diff_percent = data['ê°€ê²©ì°¨ì´_í¼ì„¼íŠ¸']
                        
                        if price_diff > 0:
                            st.metric("ê°€ê²© ì°¨ì´", f"+{price_diff:,}ì›", f"+{price_diff_percent}%")
                        else:
                            st.metric("ê°€ê²© ì°¨ì´", f"{price_diff:,}ì›", f"{price_diff_percent}%")
                        
                        # ì‹œì¥ í¬ì§€ì…˜
                        position = data['ì‹œì¥_í¬ì§€ì…˜']
                        if position == "ìµœì €ê°€ ê·¸ë£¹":
                            st.success(f"ğŸ¯ ì‹œì¥ í¬ì§€ì…˜: **{position}**")
                        elif position == "í‰ê·  ì´í•˜":
                            st.info(f"ğŸ“Š ì‹œì¥ í¬ì§€ì…˜: **{position}**")
                        else:
                            st.warning(f"ğŸ“ˆ ì‹œì¥ í¬ì§€ì…˜: **{position}**")
        else:
            st.info("ê°€ê²© ê²½ìŸë ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    with tab2:
        st.subheader(f"ğŸ“Š {category_type} ì¹´í…Œê³ ë¦¬ ì‹œì¥ ì ìœ ìœ¨")
        
        # ì‹œì¥ ì ìœ ìœ¨ ìƒì„¸
        if 'market_share' in category_data.get('business_insights', {}):
            st.markdown("#### ğŸ† ë¸Œëœë“œë³„ ì‹œì¥ ì ìœ ìœ¨ (ê³ ìœ ì œí’ˆ ê¸°ì¤€)")
            share_data = category_data['business_insights']['market_share']
            
            share_df = pd.DataFrame([
                {'ë¸Œëœë“œ': brand, 'ê³ ìœ ì œí’ˆ ìˆ˜': data['ê³ ìœ ì œí’ˆ_ìˆ˜'], 'ì ìœ ìœ¨': f"{data['ì ìœ ìœ¨_í¼ì„¼íŠ¸']}%"}
                for brand, data in share_data.items()
            ])
            
            st.dataframe(share_df, use_container_width=True)
        else:
            st.info("ì‹œì¥ ì ìœ ìœ¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    with tab3:
        # ì¹´í…Œê³ ë¦¬ ì •ë³´ ìš”ì•½
        st.markdown(f"#### ğŸ“ˆ {category_type} ì¹´í…Œê³ ë¦¬ ë¶„ì„ ìš”ì•½")
        st.json({
            "ì¹´í…Œê³ ë¦¬ëª…": category_data.get('category_name', 'Unknown'),
            "ì´_ë¶„ì„ì œí’ˆìˆ˜": category_data.get('total_products_analyzed', 0),
            "ê³ ìœ ì œí’ˆìˆ˜": category_data.get('total_unique_products', 0),
            "ì„œë¡œë¸Œëœë“œ_ì œí’ˆìˆ˜": category_data.get('our_products_count', 0),
            "ì„œë¡œë¸Œëœë“œ_ê³ ìœ ì œí’ˆìˆ˜": category_data.get('our_unique_products_count', 0),
            "ê²½ìŸì‚¬_ì œí’ˆìˆ˜": category_data.get('competitor_products_count', 0),
            "ê²½ìŸì‚¬_ê³ ìœ ì œí’ˆìˆ˜": category_data.get('competitor_unique_products_count', 0)
        })

def main():
    st.set_page_config(
        page_title="ì„œë¡œ ìˆ˜ì •ê³¼ - ì‹œì¥ ê°€ê²© ë¶„ì„",
        page_icon="ğŸ¥¤",
        layout="wide"
    )
    
    # í—¤ë”
    st.title("ğŸ¥¤ ì„œë¡œ ìˆ˜ì •ê³¼ - ì‹œì¥ ê°€ê²© ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
    st.markdown("##### *í”Œë«í¼ë³„ ê°€ê²© ê²½ìŸë ¥ ë° ì‹œì¥ í¬ì§€ì…”ë‹ ë¶„ì„*")
    
    st.markdown("---")
    
    analyzer = SujeonggwaMarketAnalyzer()
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ğŸ“Š ë¶„ì„ ì„¤ì •")
        
        uploaded_files = st.file_uploader(
            "ì—‘ì…€ íŒŒì¼ ì—…ë¡œë“œ",
            type=['xlsx', 'xls'],
            accept_multiple_files=True,
            help="ë„¤ì´ë²„, ì¿ íŒ¡, ì˜¬ì›¨ì´ì¦ˆ ìˆ˜ì •ê³¼ ê°€ê²© ë°ì´í„°"
        )
        
        st.markdown("---")
        
        if uploaded_files:
            st.success(f"âœ… {len(uploaded_files)}ê°œ íŒŒì¼ ì—…ë¡œë“œë¨")
            for file in uploaded_files:
                platform = analyzer.extract_platform_from_filename(file.name)
                st.write(f"ğŸ“„ {platform}: {file.name}")
        
        if st.button("ğŸš€ ë¶„ì„ ì‹œì‘", type="primary", disabled=not uploaded_files):
            st.session_state.run_analysis = True
        
        st.markdown("---")
        st.markdown("### ğŸ“‹ ë¶„ì„ í•­ëª©")
        st.markdown("""
        - âœ… í”Œë«í¼ë³„ ê°€ê²© ê²½ìŸë ¥
        - âœ… ì‹œì¥ í¬ì§€ì…”ë‹ ë¶„ì„  
        - âœ… ë¸Œëœë“œë³„ ì ìœ ìœ¨
        - âœ… ê°€ê²©ëŒ€ë³„ ì œí’ˆ ë¶„í¬
        """)

    # ë©”ì¸ ë¶„ì„
    if uploaded_files and st.session_state.get('run_analysis', False):
        
        # í”„ë¡œê·¸ë ˆìŠ¤ í‘œì‹œ
        progress_container = st.container()
        with progress_container:
            progress_bar = st.progress(0)
            status_text = st.empty()
        
        df_list = []
        platform_info = []
        
        # íŒŒì¼ ì²˜ë¦¬
        for i, uploaded_file in enumerate(uploaded_files):
            status_text.text(f"ğŸ“‚ íŒŒì¼ ì²˜ë¦¬ ì¤‘: {uploaded_file.name}")
            progress_bar.progress((i + 1) / len(uploaded_files) * 0.4)
            
            df, platform, missing_cols = analyzer.load_and_standardize_excel(uploaded_file)
            
            if df is not None:
                df_list.append(df)
                platform_info.append({
                    'filename': uploaded_file.name,
                    'platform': platform,
                    'rows': len(df),
                    'missing_columns': missing_cols
                })
        
        if df_list:
            status_text.text("ğŸ” ì‹œì¥ ë°ì´í„° ë¶„ì„ ì¤‘...")
            progress_bar.progress(0.7)
            
            # í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„
            analysis_results, handmade_df, all_products_df = analyzer.analyze_business_critical_data(df_list)
            
            status_text.text("ğŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")
            progress_bar.progress(0.9)
            
            # GitHubì— ìë™ ì €ì¥
            status_text.text("ğŸ’¾ GitHubì— ì €ì¥ ì¤‘...")
            
            # ê¸°ì¡´ ê²°ê³¼ íŒŒì¼ë“¤ ì‚­ì œ
            analyzer.clear_github_results()
            
            # ìƒˆ ê²°ê³¼ ì €ì¥
            json_content = json.dumps(analysis_results, ensure_ascii=False, indent=2)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            json_filename = f"analysis_results_{timestamp}.json"
            
            github_success = analyzer.save_to_github(json_content, json_filename)
            
            progress_bar.progress(1.0)
            status_text.empty()
            progress_container.empty()
            
            # ê²°ê³¼ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
            st.session_state.analysis_results = analysis_results
            st.session_state.json_content = json_content
            st.session_state.timestamp = timestamp
            
            # ê²°ê³¼ ëŒ€ì‹œë³´ë“œ í‘œì‹œ
            show_analysis_results(analysis_results, json_content, timestamp, github_success)
            
            # ì„¸ì…˜ ìƒíƒœ ë¦¬ì…‹
            st.session_state.run_analysis = False
    
    # ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ê°€ ì„¸ì…˜ì— ìˆê±°ë‚˜ GitHubì—ì„œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆëŠ” ê²½ìš°
    elif st.session_state.get('analysis_results') or not uploaded_files:
        
        # ì„¸ì…˜ì— ê²°ê³¼ê°€ ì—†ìœ¼ë©´ GitHubì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
        if not st.session_state.get('analysis_results'):
            with st.spinner("GitHubì—ì„œ ìµœì‹  ë¶„ì„ ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
                latest_analysis = analyzer.load_latest_analysis_from_github()
                
                if latest_analysis:
                    st.session_state.analysis_results = latest_analysis
                    st.session_state.json_content = json.dumps(latest_analysis, ensure_ascii=False, indent=2)
                    st.session_state.timestamp = latest_analysis.get('timestamp', 'unknown')
                    st.success("âœ… GitHubì—ì„œ ìµœì‹  ë¶„ì„ ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤!")
        
        # ë¶„ì„ ê²°ê³¼ í‘œì‹œ
        if st.session_state.get('analysis_results'):
            show_analysis_results(
                st.session_state.analysis_results, 
                st.session_state.get('json_content', ''), 
                st.session_state.get('timestamp', 'unknown'),
                True
            )
        else:
            # ì´ˆê¸° í™”ë©´  
            st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ì—‘ì…€ íŒŒì¼ë“¤ì„ ì—…ë¡œë“œí•˜ê³  ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.")
            
            # ê°„ë‹¨í•œ ì•ˆë‚´ ë©”ì‹œì§€
            with st.expander("ğŸ“‹ ì‚¬ìš© ë°©ë²•", expanded=False):
                st.markdown("""
                ### ğŸš€ ì£¼ìš” ê¸°ëŠ¥
                
                **ğŸ¯ í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„**
                - ì„œë¡œ ë¸Œëœë“œì˜ í”Œë«í¼ë³„ ê°€ê²© ê²½ìŸë ¥ ë¶„ì„
                - ë‹¨ìœ„ê°€ê²©(100mlë‹¹) ê¸°ì¤€ ì‹œì¥ í¬ì§€ì…”ë‹  
                - ê²½ìŸì‚¬ ëŒ€ë¹„ ê°€ê²© ì°¨ì´ ë° ê²½ìŸë ¥ í‰ê°€
                
                **ğŸ“Š ì‹œì¥ í˜„í™© íŒŒì•…**
                - ë¸Œëœë“œë³„ ì‹œì¥ ì ìœ ìœ¨ ë¶„ì„
                - ê°€ê²©ëŒ€ë³„ ì œí’ˆ ë¶„í¬ ë° ìš°ë¦¬ ë¸Œëœë“œ ìœ„ì¹˜
                - í”Œë«í¼ë³„ ì œí’ˆ í˜„í™© ë¹„êµ
                
                ### ğŸ“ íŒŒì¼ ì—…ë¡œë“œ ê°€ì´ë“œ
                - **ì§€ì› í˜•ì‹**: Excel íŒŒì¼ (.xlsx, .xls)
                - **íŒŒì¼ëª… ì˜ˆì‹œ**: "ë„¤ì´ë²„ ìˆ˜ì •ê³¼ ê°€ê²©", "ì¿ íŒ¡ ìˆ˜ì •ê³¼ ê°€ê²©" ë“±
                - **í•„ìˆ˜ ì»¬ëŸ¼**: ë¸Œëœë“œ, ì œí’ˆëª…, ìš©ëŸ‰(ml), ìµœì €ê°€(ë°°ì†¡ë¹„ í¬í•¨), ìµœì €ê°€ ë‹¨ìœ„ê°€ê²©(100mlë‹¹) ë“±
                """)

# Streamlit ì•± ì‹¤í–‰
if __name__ == "__main__":
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'run_analysis' not in st.session_state:
        st.session_state.run_analysis = False
    
    main()
